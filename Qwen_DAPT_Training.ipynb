{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling,\n",")\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n",")\n","\n","# --------------------------------------------------------------------------------\n","# 1. 경로 및 환경 설정\n","# --------------------------------------------------------------------------------\n","BASE_PATH = \"/content/drive/MyDrive/textanl\"\n","DATA_PATH = os.path.join(BASE_PATH, \"dataset/df.jsonl\")\n","OUTPUT_DIR = BASE_PATH\n","\n","# FP8 말고 일반 인스트럭트 버전 사용 (양자화 안 쓰는 세팅에 더 적합)\n","MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","\n","print(f\"[-] Model ID: {MODEL_ID}\")\n","print(f\"[-] Mode: No quantization (pure BF16 full model) + LoRA\")\n","\n","# 약간의 성능 튜닝 (선택)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","\n","# --------------------------------------------------------------------------------\n","# 2. Config 로드 (혹시 남아 있을 수 있는 quantization_config 정리)\n","# --------------------------------------------------------------------------------\n","config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n","\n","# 안전용 방어 로직 (일반 인스트럭트 모델이면 보통 필요 없지만, 있어도 무해)\n","if hasattr(config, \"quantization_config\"):\n","    del config.quantization_config\n","    print(\"[-] Removed existing quantization_config from config.\")\n","\n","# --------------------------------------------------------------------------------\n","# 3. 모델 및 토크나이저 로드 (BF16, 비양자화)\n","# --------------------------------------------------------------------------------\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID,\n","    config=config,\n","    device_map=\"auto\",                # 단일 GPU면 자동으로 그쪽에 실림\n","    torch_dtype=torch.bfloat16,       # A100 BF16 사용\n","    trust_remote_code=True,\n",")\n","\n","# DDP/GC와 호환 위해 캐시 비활성화\n","model.config.use_cache = False\n","model.gradient_checkpointing_enable()\n","\n","# --------------------------------------------------------------------------------\n","# 4. LoRA 어댑터 설정\n","# --------------------------------------------------------------------------------\n","# 양자화는 안 쓰지만, 4B 전체 full-finetune은 굳이 안 해도 되므로 LoRA는 유지\n","# 조금 더 보수적으로 가려면 r 값을 32 정도로 줄여도 됨.\n","peft_config = LoraConfig(\n","    r=64,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",")\n","\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","# --------------------------------------------------------------------------------\n","# 5. 데이터셋 로드 및 토크나이징\n","# --------------------------------------------------------------------------------\n","dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n","\n","def tokenize_function(examples):\n","    return tokenizer(\n","        examples[\"text\"],\n","        truncation=True,\n","        max_length=1024,\n","        padding=False,\n","    )\n","\n","print(\"[-] Tokenizing dataset...\")\n","tokenized_datasets = dataset.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=[\"text\"],\n",")\n","\n","# --------------------------------------------------------------------------------\n","# 6. 학습 설정 (양자화 없는 BF16 기준)\n","# --------------------------------------------------------------------------------\n","# 양자화 안 쓰면 VRAM 부담이 커지니까 배치는 보수적으로 시작하는 게 안전함.\n","# A100 80GB 기준이면 아래 값 정도는 무난한 편 (seq_len=2048 기준):\n","#   - per_device_train_batch_size = 4\n","#   - gradient_accumulation_steps = 4  -> effective batch = 16\n","training_args = TrainingArguments(\n","    output_dir=os.path.join(OUTPUT_DIR, \"qwen3_checkpoints_bf16_lora\"),\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    learning_rate=2e-4,                  # DAPT + LoRA에는 이 정도도 가능\n","    num_train_epochs=1,\n","    logging_steps=1000,\n","    fp16=False,\n","    bf16=True,                           # BF16 사용\n","    optim=\"adamw_torch\",                 # bnb 의존성 없는 기본 AdamW\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.03,\n","    save_strategy=\"epoch\",\n","    report_to=\"none\",\n","    ddp_find_unused_parameters=False,\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    train_dataset=tokenized_datasets,\n","    args=training_args,\n","    data_collator=data_collator,\n",")\n","\n","# --------------------------------------------------------------------------------\n","# 7. 학습 실행 및 저장\n","# --------------------------------------------------------------------------------\n","print(\"[-] Starting Training (no quantization, BF16 + LoRA)...\")\n","trainer.train()\n","\n","save_path = os.path.join(OUTPUT_DIR, \"qwen2.5_1.5b_dapt_adapter_bf16\")\n","trainer.model.save_pretrained(save_path)\n","tokenizer.save_pretrained(save_path)\n","print(f\"[-] Model saved to: {save_path}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[-] Model ID: Qwen/Qwen2.5-1.5B-Instruct\n","[-] Mode: No quantization (pure BF16 full model) + LoRA\n","trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"Unable to find '/content/drive/MyDrive/textanl/dataset/df.jsonl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2616696913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# 5. 데이터셋 로드 및 토크나이징\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         ).get_module()\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;31m# Try locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         )\n\u001b[0;32m--> 526\u001b[0;31m         data_files = DataFilesDict.from_patterns(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFilesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 else DataFilesList.from_patterns(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 data_files.extend(\n\u001b[0;32m--> 582\u001b[0;31m                     resolve_pattern(\n\u001b[0m\u001b[1;32m    583\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                         \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/content/drive/MyDrive/textanl/dataset/df.jsonl'"]}],"execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"aCRJ6wE59LBV","outputId":"58c1e091-a1f0-4f52-b74d-f3d9764ad44d","executionInfo":{"status":"error","timestamp":1764081886186,"user_tz":-540,"elapsed":5082,"user":{"displayName":"YZ X","userId":"18036600658637177641"}}}},{"cell_type":"code","source":[],"metadata":{"id":"eVqMcpK7-2uQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}