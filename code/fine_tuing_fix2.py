#!/usr/bin/env python
# coding: utf-8

# In[ ]:


from google.colab import drive
drive.mount('/content/drive')

get_ipython().system('pip install evaluate rouge_score -q')

import os
import torch
import numpy as np
import json
import evaluate
import matplotlib.pyplot as plt
import re

from datasets import load_dataset
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import (
    PeftModel,
    LoraConfig,
    get_peft_model,
)

# matplotlib ë°±ì—”ë“œ ì„¤ì • (ë…¸íŠ¸ë¶ í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš° ì˜¤ë¥˜ ë°©ì§€)
plt.switch_backend('Agg')
rouge = evaluate.load("rouge")

# --------------------------------------------------------------------------------
# 1. ê²½ë¡œ ë° í™˜ê²½ ì„¤ì • (ìœ ì§€)
# --------------------------------------------------------------------------------
BASE_PATH = "/content/drive/MyDrive/textanl"
TRAIN_DATA_FILE = os.path.join(BASE_PATH, "dataset/train_instruction_corpus.jsonl")
VALID_DATA_FILE = os.path.join(BASE_PATH, "dataset/valid_instruction_corpus.jsonl")
OUTPUT_DIR = os.path.join(BASE_PATH, "qa")
MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"
LORA_PATH = os.path.join(BASE_PATH, "dapt_params/qwen2.5_1.5b_dapt_adapter_bf16")

MAX_LENGTH = 512
PROMPT_SEPARATOR_WITH_NEWLINE = "### Assistant:\n" # ë§ˆìŠ¤í‚¹ ê¸°ì¤€ êµ¬ë¶„ì

# --------------------------------------------------------------------------------
# 2. DAPTëœ ë² ì´ìŠ¤ ëª¨ë¸ + ê¸°ì¡´ LoRA ë¡œë“œ
# --------------------------------------------------------------------------------

# âœ… í† í¬ë‚˜ì´ì €ëŠ” ë² ì´ìŠ¤ ëª¨ë¸ì—ì„œ
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_ID,
    trust_remote_code=True,
    fix_mistral_regex=True,
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# 1) ìˆœì • Qwen2.5 ë² ì´ìŠ¤ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    torch_dtype=torch.float16,   # bf16 ì“°ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ë‘ TrainingArguments ë§ì¶”ë©´ ë¨
    trust_remote_code=True,
)

# 2) ê¸°ì¡´ì— í•™ìŠµí•œ LoRA ì–´ëŒ‘í„° ë¡œë“œ (ì´ê±¸ ê·¸ëŒ€ë¡œ ì´ì–´ì„œ í•™ìŠµ)
model = PeftModel.from_pretrained(
    base_model,
    LORA_PATH,
    is_trainable=True,
)

model.config.use_cache = False
model.print_trainable_parameters()
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

# --------------------------------------------------------------------------------
# 4. ë°ì´í„°ì…‹ ë¡œë“œ ë° í† í¬ë‚˜ì´ì§• (ğŸ“Œ ë§ˆìŠ¤í‚¹ ë° í´ë¦¬ë‹ ê°•í™”)
# --------------------------------------------------------------------------------
raw_datasets = load_dataset(
    "json",
    data_files={"train": TRAIN_DATA_FILE, "validation": VALID_DATA_FILE}
)

def clean_input_text(text):
    """
    Input í…ìŠ¤íŠ¸ ë‚´ì˜ ëª¨ë“  ê°œí–‰ ë¬¸ì(\n)ì™€ ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
    """
    if not isinstance(text, str):
        return ""
    # ğŸ“Œ \n, \t, ì—°ì† ê³µë°±ì„ ëª¨ë‘ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def tokenize_and_mask_function(examples):
    full_text = examples["text"]

    # 1. í…ìŠ¤íŠ¸ í´ë¦¬ë‹ ì ìš©: \n ë¬¸ìë¥¼ ì œê±°í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ëˆ
    full_text = clean_input_text(full_text)

    # 2. ì‹œí€€ìŠ¤ í† í°í™”
    tokenized = tokenizer(
        full_text, truncation=True, max_length=MAX_LENGTH, padding=False,
    )
    labels = tokenized["input_ids"].copy()

    # --- Masking Logic: í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì˜ labelsë¥¼ -100ìœ¼ë¡œ ì„¤ì • ---
    # êµ¬ë¶„ìë„ í´ë¦¬ë‹ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í´ë¦¬ë‹ëœ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ë¶„ìë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    # PROMPT_SEPARATOR_WITH_NEWLINE ("### Assistant:\n")ì˜ \nì€ í´ë¦¬ë‹ì— ì˜í•´ ê³µë°±ìœ¼ë¡œ ë°”ë€ë‹ˆë‹¤.
    CLEANED_SEPARATOR = clean_input_text(PROMPT_SEPARATOR_WITH_NEWLINE) # "### Assistant:"

    parts = full_text.split(CLEANED_SEPARATOR, 1)

    if len(parts) == 2:
        prompt_plus_header = parts[0] + CLEANED_SEPARATOR

        # í”„ë¡¬í”„íŠ¸ + í—¤ë”ë¥¼ í† í°í™”í•˜ì—¬ ê¸¸ì´ë¥¼ ì •í™•íˆ ê³„ì‚°
        # add_special_tokens=Falseë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ê°œìˆ˜ ê³„ì‚°
        prompt_tokens = tokenizer(
            prompt_plus_header,
            add_special_tokens=False,
            truncation=True,
            max_length=MAX_LENGTH,
            padding=False
        )
        prompt_len = len(prompt_tokens["input_ids"])

        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë§Œí¼ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
        if prompt_len < len(labels):
            labels[:prompt_len] = [-100] * prompt_len
        else:
            labels[:] = [-100] * len(labels)
    else:
        labels[:] = [-100] * len(labels)

    tokenized["labels"] = labels
    return tokenized

print(f"[-] Tokenizing dataset and applying label masking (Input Cleaned)...")
tokenized_datasets = raw_datasets.map(
    tokenize_and_mask_function, batched=False, remove_columns=["text"],
)

train_dataset = tokenized_datasets["train"]
eval_dataset = tokenized_datasets["validation"]


# --------------------------------------------------------------------------------
# 5. í•™ìŠµ ì„¤ì • (TrainingArguments)
# --------------------------------------------------------------------------------

# ROUGE í•„í„°ë§ í•¨ìˆ˜ (í‰ê°€ ì‹œ ìˆœìˆ˜ ë‹µë³€ ì¶”ì¶œìš©)
def filter_generated_text(text, separator):
    # separatorëŠ” í´ë¦¬ë‹ëœ "### Assistant:" ì…ë‹ˆë‹¤.
    CLEANED_SEPARATOR = re.sub(r'\s+', ' ', separator).strip()

    if CLEANED_SEPARATOR in text:
        filtered_text = text.split(CLEANED_SEPARATOR, 1)[1].strip()
        filtered_text = filtered_text.replace("### Human:", "").strip()
        filtered_text = filtered_text.replace("### Assistant:", "").strip()
        return filtered_text
    return ""

# compute_metrics í•¨ìˆ˜ ì •ì˜ (Lossì™€ ROUGE-L ê³„ì‚°)
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    global tokenizer
    global rouge

    predictions = np.array(predictions)

    # ğŸ”¹ logits(T,B,V)ë¡œ ë“¤ì–´ì˜¤ë©´ argmax, ì´ë¯¸ ids(B,T)ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if predictions.ndim == 3:
        predictions = np.argmax(predictions, axis=-1)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds_filtered = [filter_generated_text(pred, PROMPT_SEPARATOR_WITH_NEWLINE) for pred in decoded_preds]
    decoded_labels_filtered = [filter_generated_text(label, PROMPT_SEPARATOR_WITH_NEWLINE) for label in decoded_labels]

    rouge_results = rouge.compute(
        predictions=decoded_preds_filtered,
        references=decoded_labels_filtered
    )
    return {"rougeL": rouge_results["rougeL"]}



training_args = TrainingArguments(
    output_dir=os.path.join(OUTPUT_DIR, "qwen_r32_ift_checkpoints"),

    per_device_train_batch_size=8,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,

    learning_rate=5e-5,
    num_train_epochs=3,

    logging_strategy="steps",
    logging_steps=1000,             # ğŸ”¹ ì—¬ê¸°ì„œ train loss ì°í˜

    fp16=True,
    bf16=False,

    optim="adamw_torch",
    lr_scheduler_type="cosine",
    warmup_ratio=0.06,

    do_eval=True,
    eval_strategy="steps",          # ğŸ”¹ ìŠ¤í… ë‹¨ìœ„ë¡œ val loss ê³„ì‚°
    eval_steps=1000,                # ğŸ”¹ 1000 stepë§ˆë‹¤ eval_loss ì°í˜

    save_strategy="steps",
    save_steps=1000,

    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",  # ğŸ”¹ ì´ì œ ê¸°ì¤€ì€ eval_loss
    greater_is_better=False,

    logging_dir=os.path.join(OUTPUT_DIR, "qwen_r32_ift_checkpoints", "logs"),
    report_to="none",
    gradient_checkpointing=False,
)

from dataclasses import dataclass
from typing import Dict, List, Any
import torch
from transformers import PreTrainedTokenizerBase

@dataclass
class DataCollatorForCausalLMWithMaskedLabels:
    tokenizer: PreTrainedTokenizerBase
    max_length: int = 1024

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        # 1) labels ë”°ë¡œ ë¹¼ë‘ê¸°
        labels = [f["labels"] for f in features]

        # 2) tokenizer.padì— ë„˜ê¸¸ ë•ŒëŠ” labels ë¹¼ê³  ë„˜ê¸°ê¸°
        features_no_labels = []
        for f in features:
            f = dict(f)         # shallow copy
            f.pop("labels")     # labels ì œê±°
            features_no_labels.append(f)

        # 3) input_ids / attention_mask íŒ¨ë”©
        batch = self.tokenizer.pad(
            features_no_labels,
            padding="longest",
            max_length=self.max_length,
            return_tensors="pt",
        )

        # 4) labelsë„ ê°™ì€ ê¸¸ì´ë¡œ -100 íŒ¨ë”©
        max_len = batch["input_ids"].shape[1]
        padded_labels = torch.full(
            (len(labels), max_len),
            -100,
            dtype=torch.long,
        )

        for i, l in enumerate(labels):
            l = l[:max_len]
            padded_labels[i, :len(l)] = torch.tensor(l, dtype=torch.long)

        batch["labels"] = padded_labels
        return batch

data_collator = DataCollatorForCausalLMWithMaskedLabels(
    tokenizer=tokenizer,
    max_length=MAX_LENGTH,
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=training_args,
    data_collator=data_collator,
    compute_metrics=None,
)

# --------------------------------------------------------------------------------
# 6. í•™ìŠµ ì‹¤í–‰ ë° ì €ì¥
# --------------------------------------------------------------------------------
print("---íŒŒì¸íŠœë‹ ì‹œì‘--- (r=32, Input Cleaned, Logging ROUGE-L)")
trainer.train()

save_path = os.path.join(OUTPUT_DIR, "qa_params")
trainer.model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
print(f"[-] IFT Model saved to: {save_path}")

# --------------------------------------------------------------------------------
# 7. í•™ìŠµ ë¡œê·¸ ì €ì¥ (ì‹œê°í™” ë°ì´í„°ë¥¼ ìœ„í•œ ì¶”ì¶œ)
# --------------------------------------------------------------------------------
print("\n[--- Saving Training and Evaluation Logs ì €ì¥ì¤‘---]")

log_history = trainer.state.log_history
log_output_path = os.path.join(OUTPUT_DIR, "qwen_r32_ift_metrics_log.json")

with open(log_output_path, 'w', encoding='utf-8') as f:
    json.dump(log_history, f, ensure_ascii=False, indent=4)

print(f"âœ… í•™ìŠµ/í‰ê°€ ë¡œê·¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ì™„ë£Œ: {log_output_path}")

print("\në¡œê·¸ í™œìš© ê°€ì´ë“œ]")
print(f"ì €ì¥ëœ {os.path.basename(log_output_path)} íŒŒì¼ì„ ì´ìš©í•˜ì—¬ 'loss', 'eval_loss', 'eval_rougeL' ê°’ì„ ì¶”ì¶œí•˜ì—¬ ì‹œê°í™” ê°€ëŠ¥.")


# In[ ]:


from google.colab import drive
drive.mount('/content/drive')


# In[ ]:


from torch.utils.data import DataLoader
import numpy as np

print("\n[--- ìµœì¢… ê²€ì¦: validation ì…‹ ROUGE-L í•œ ë²ˆ ê³„ì‚° ---]")

model.eval()
eval_loader = DataLoader(
    eval_dataset,
    batch_size=1,           # ì•ˆì „í•˜ê²Œ 1
    shuffle=False,
    collate_fn=data_collator,
)

all_preds_text = []
all_labels_text = []

for batch in eval_loader:
    batch = {k: v.to(model.device) for k, v in batch.items()}

    with torch.no_grad():
        outputs = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
        )
        logits = outputs.logits          # (1, L, V)

    # (1, L) â†’ (L,)
    pred_ids  = torch.argmax(logits, dim=-1).cpu().numpy()[0]
    label_ids = batch["labels"].cpu().numpy()[0]

    # ğŸ”¹ ë‹µë³€ í† í° ìœ„ì¹˜ë§Œ ì‚¬ìš© (í”„ë¡¬í”„íŠ¸ / í—¤ë”ëŠ” ì´ë¯¸ -100)
    mask = label_ids != -100
    if not np.any(mask):
        continue  # í˜¹ì‹œ ì „ë¶€ -100ì¸ ì´ìƒí•œ ìƒ˜í”Œ ìˆìœ¼ë©´ ìŠ¤í‚µ

    pred_ans_ids  = pred_ids[mask]
    label_ans_ids = label_ids[mask]

    # ğŸ”¹ labelì˜ -100ì€ ì´ë¯¸ ë§ˆìŠ¤í¬ë¡œ ê±¸ë €ìœ¼ë‹ˆ ì¶”ê°€ ì¹˜í™˜ í•„ìš” ì—†ìŒ
    #    (í˜¹ì‹œ ì•ˆì „í•˜ê²Œ í•˜ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ì²˜ëŸ¼ í•œ ë²ˆ ë”)
    # label_ans_ids = np.where(label_ans_ids != -100, label_ans_ids, tokenizer.pad_token_id)

    pred_text  = tokenizer.decode(pred_ans_ids,  skip_special_tokens=True)
    label_text = tokenizer.decode(label_ans_ids, skip_special_tokens=True)

    # ğŸ”¹ ì—¬ê¸°ì„œëŠ” ë” ì´ìƒ filter_generated_text ì“°ì§€ ë§ ê²ƒ
    all_preds_text.append(pred_text.strip())
    all_labels_text.append(label_text.strip())

# ğŸ”¹ ROUGE ê³„ì‚°
final_metrics = rouge.compute(
    predictions=all_preds_text,
    references=all_labels_text,
)

print(f"âœ… ìµœì¢… ROUGE-L: {final_metrics['rougeL']:.4f}")


# In[ ]:




